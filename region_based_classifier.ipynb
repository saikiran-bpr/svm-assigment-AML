{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "63d94193",
            "metadata": {},
            "outputs": [],
            "source": [
                "pip install opencv-python numpy scikit-learn joblib scipy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f8a1b2c3",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Building region dataset for training...\n",
                        "Processing 50 images with -1 jobs...\n",
                        "Built train regions: 10000 regions from 50 images (time 61.3s)\n",
                        "Training SVM...\n",
                        "Trained SVM in 127.60s\n",
                        "Building region dataset for validation...\n",
                        "Processing 20 images with -1 jobs...\n",
                        "\n",
                        "=== Optimized Method 3: Region-based (KMeans -> SVM) ===\n",
                        "Accuracy : 0.9490\n",
                        "Precision: 0.0000\n",
                        "Recall   : 0.0000\n",
                        "Region build (train) time: 61.27s\n",
                        "SVM training time:         127.60s\n",
                        "Region build (val) time:   25.52s\n",
                        "Inference time:            0.05s\n",
                        "Visualization skipped in batch mode for simplicity.\n"
                    ]
                }
            ],
            "source": [
                "import cv2\n",
                "import numpy as np\n",
                "import glob\n",
                "import time\n",
                "from sklearn.cluster import KMeans\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
                "from joblib import Parallel, delayed\n",
                "import scipy.ndimage as ndimage\n",
                "\n",
                "TRAIN_IMG_DIR = \"./ADEChallengeData2016/images/training/\"\n",
                "TRAIN_ANN_DIR = \"./ADEChallengeData2016/annotations/training/\"\n",
                "VAL_IMG_DIR   = \"./ADEChallengeData2016/images/validation/\"\n",
                "VAL_ANN_DIR   = \"./ADEChallengeData2016/annotations/validation/\"\n",
                "\n",
                "FLOOR_ID = 4\n",
                "\n",
                "DOWNSCALE_MAX = 300\n",
                "CLUSTERS_PER_IMAGE = 200\n",
                "\n",
                "NUM_TRAIN_IMAGES = 50\n",
                "NUM_VAL_IMAGES = 20\n",
                "\n",
                "SVM_KERNEL = \"linear\"\n",
                "SVM_C = 1.0\n",
                "N_JOBS = -1\n",
                "\n",
                "def downscale_keep_aspect(img, max_dim):\n",
                "    h, w = img.shape[:2]\n",
                "    scale = 1.0\n",
                "    if max(h, w) > max_dim:\n",
                "        scale = max_dim / max(h, w)\n",
                "        img = cv2.resize(img, (int(w * scale), int(h * scale)), interpolation=cv2.INTER_AREA)\n",
                "    return img, scale\n",
                "\n",
                "def process_single_image(img_path, ann_path, n_clusters, max_dim):\n",
                "    try:\n",
                "        img = cv2.imread(img_path)\n",
                "        ann = cv2.imread(ann_path, 0)\n",
                "        if img is None or ann is None: return None\n",
                "\n",
                "\n",
                "        img_ds, scale = downscale_keep_aspect(img, max_dim)\n",
                "        ann_ds = cv2.resize(ann, (img_ds.shape[1], img_ds.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
                "        \n",
                "        H, W = img_ds.shape[:2]\n",
                "        \n",
                "\n",
                "        rgb = img_ds.reshape(-1, 3).astype(np.float32)\n",
                "        xs, ys = np.meshgrid(np.arange(W), np.arange(H))\n",
                "        xs_flat = xs.reshape(-1)\n",
                "        ys_flat = ys.reshape(-1)\n",
                "        \n",
                "\n",
                "        xs_norm = xs_flat / W\n",
                "        ys_norm = ys_flat / H\n",
                "        \n",
                "        cluster_input = np.hstack([rgb, xs_norm.reshape(-1, 1), ys_norm.reshape(-1, 1)])\n",
                "\n",
                "\n",
                "        km = KMeans(n_clusters=n_clusters, random_state=42, n_init=4)\n",
                "        labels_flat = km.fit_predict(cluster_input)\n",
                "        labels_grid = labels_flat.reshape(H, W)\n",
                "\n",
                "\n",
                "        present_clusters = np.unique(labels_flat)\n",
                "        \n",
                "\n",
                "        floor_mask_flat = (ann_ds.reshape(-1) == FLOOR_ID).astype(np.float32)\n",
                "        \n",
                "\n",
                "        mean_r = ndimage.mean(rgb[:, 0], labels=labels_flat, index=present_clusters)\n",
                "        mean_g = ndimage.mean(rgb[:, 1], labels=labels_flat, index=present_clusters)\n",
                "        mean_b = ndimage.mean(rgb[:, 2], labels=labels_flat, index=present_clusters)\n",
                "        \n",
                "\n",
                "        std_r = ndimage.standard_deviation(rgb[:, 0], labels=labels_flat, index=present_clusters)\n",
                "        std_g = ndimage.standard_deviation(rgb[:, 1], labels=labels_flat, index=present_clusters)\n",
                "        std_b = ndimage.standard_deviation(rgb[:, 2], labels=labels_flat, index=present_clusters)\n",
                "        \n",
                "\n",
                "        mean_x = ndimage.mean(xs_norm, labels=labels_flat, index=present_clusters)\n",
                "        mean_y = ndimage.mean(ys_norm, labels=labels_flat, index=present_clusters)\n",
                "        \n",
                "\n",
                "        counts = ndimage.histogram(labels_flat, min=0, max=n_clusters-1, bins=n_clusters)\n",
                "        area_frac = counts[present_clusters] / float(H * W)\n",
                "        \n",
                "        floor_fractions = ndimage.mean(floor_mask_flat, labels=labels_flat, index=present_clusters)\n",
                "        region_labels = (floor_fractions >= 0.5).astype(np.int32)\n",
                "        \n",
                "        std_sum = std_r + std_g + std_b\n",
                "        features = np.column_stack([mean_r, mean_g, mean_b, std_sum, mean_x, mean_y, area_frac])\n",
                "        \n",
                "        return features, region_labels, (img_ds, ann_ds, labels_grid)\n",
                "\n",
                "    except Exception as e:\n",
                "        print(f\"Error: {e}\")\n",
                "        return None\n",
                "\n",
                "def build_dataset_parallel(image_paths, ann_paths, n_clusters, max_dim):\n",
                "    print(f\"Processing {len(image_paths)} images with {N_JOBS} jobs...\")\n",
                "    results = Parallel(n_jobs=N_JOBS)(delayed(process_single_image)(img, ann, n_clusters, max_dim) \n",
                "                                      for img, ann in zip(image_paths, ann_paths))\n",
                "    \n",
                "    results = [r for r in results if r is not None]\n",
                "    \n",
                "    if not results:\n",
                "        return np.array([]), np.array([]), []\n",
                "        \n",
                "    X_list = [r[0] for r in results]\n",
                "    y_list = [r[1] for r in results]\n",
                "    maps   = [r[2] for r in results]\n",
                "    \n",
                "    return np.vstack(X_list), np.concatenate(y_list), maps\n",
                "\n",
                "\n",
                "train_images = sorted(glob.glob(TRAIN_IMG_DIR + \"*.jpg\"))[:NUM_TRAIN_IMAGES]\n",
                "train_annots = sorted(glob.glob(TRAIN_ANN_DIR + \"*.png\"))[:NUM_TRAIN_IMAGES]\n",
                "\n",
                "print(\"Building region dataset for training...\")\n",
                "t0 = time.time()\n",
                "X_train, y_train, _ = build_dataset_parallel(train_images, train_annots, CLUSTERS_PER_IMAGE, DOWNSCALE_MAX)\n",
                "build_train_time = time.time() - t0\n",
                "print(f\"Built train regions: {X_train.shape[0]} regions from {len(train_images)} images (time {build_train_time:.1f}s)\")\n",
                "\n",
                "print(\"Training SVM...\")\n",
                "svm = SVC(kernel=SVM_KERNEL, C=SVM_C)\n",
                "t0 = time.time()\n",
                "svm.fit(X_train, y_train)\n",
                "train_time = time.time() - t0\n",
                "print(f\"Trained SVM in {train_time:.2f}s\")\n",
                "\n",
                "\n",
                "val_images = sorted(glob.glob(VAL_IMG_DIR + \"*.jpg\"))[:NUM_VAL_IMAGES]\n",
                "val_annots = sorted(glob.glob(VAL_ANN_DIR + \"*.png\"))[:NUM_VAL_IMAGES]\n",
                "\n",
                "print(\"Building region dataset for validation...\")\n",
                "t0 = time.time()\n",
                "X_val, y_val, val_region_maps = build_dataset_parallel(val_images, val_annots, CLUSTERS_PER_IMAGE, DOWNSCALE_MAX)\n",
                "build_val_time = time.time() - t0\n",
                "\n",
                "t0 = time.time()\n",
                "y_pred = svm.predict(X_val)\n",
                "inference_time = time.time() - t0\n",
                "\n",
                "accuracy  = accuracy_score(y_val, y_pred)\n",
                "precision = precision_score(y_val, y_pred, zero_division=0)\n",
                "recall    = recall_score(y_val, y_pred, zero_division=0)\n",
                "\n",
                "print(\"\\n=== Optimized Method 3: Region-based (KMeans -> SVM) ===\")\n",
                "print(f\"Accuracy : {accuracy:.4f}\")\n",
                "print(f\"Precision: {precision:.4f}\")\n",
                "print(f\"Recall   : {recall:.4f}\")\n",
                "print(f\"Region build (train) time: {build_train_time:.2f}s\")\n",
                "print(f\"SVM training time:         {train_time:.2f}s\")\n",
                "print(f\"Region build (val) time:   {build_val_time:.2f}s\")\n",
                "print(f\"Inference time:            {inference_time:.2f}s\")\n",
                "\n",
                "\n",
                "if len(val_region_maps) > 0:\n",
                "    img_ds, ann_ds, labels_grid_val = val_region_maps[0]\n",
                "    H, W = img_ds.shape[:2]\n",
                "    \n",
                "\n",
                "    print(\"Visualization skipped in batch mode for simplicity.\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
